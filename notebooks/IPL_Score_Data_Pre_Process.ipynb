{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a36a12e"
   },
   "source": [
    "## Load and Combine Datasets\n",
    "\n",
    "### Subtask:\n",
    "Load 'ipl_2025_deliveries.csv', 'ipl_2024_deliveries.csv', 'ipl_2023_deliveries.csv', and 'ipl_2022_deliveries.csv' into separate DataFrames and then concatenate them into a single DataFrame. If any file is not found, it should print an error message but continue with the found files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 201,
     "status": "ok",
     "timestamp": 1764395461188,
     "user": {
      "displayName": "Arsh AJ",
      "userId": "04800759354100934554"
     },
     "user_tz": -240
    },
    "id": "4e6a52a6",
    "outputId": "0e7db404-ef24-454c-b983-cf465069998f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset 'ipl_2025_deliveries.csv' loaded successfully! Shape: (17246, 21)\n",
      "✅ Dataset 'ipl_2024_deliveries.csv' loaded successfully! Shape: (17053, 20)\n",
      "✅ Dataset 'ipl_2023_deliveries.csv' loaded successfully! Shape: (17386, 20)\n",
      "✅ Dataset 'ipl_2022_deliveries.csv' loaded successfully! Shape: (17912, 20)\n",
      "\n",
      "✅ All available datasets combined successfully!\n",
      "Combined DataFrame shape: (69597, 21)\n",
      "First 5 rows of the combined DataFrame:\n",
      "   match_id  season        phase  match_no          date  \\\n",
      "0    202501    2025  Group Stage         1  Mar 22, 2025   \n",
      "1    202501    2025  Group Stage         1  Mar 22, 2025   \n",
      "2    202501    2025  Group Stage         1  Mar 22, 2025   \n",
      "3    202501    2025  Group Stage         1  Mar 22, 2025   \n",
      "4    202501    2025  Group Stage         1  Mar 22, 2025   \n",
      "\n",
      "                   venue batting_team bowling_team  innings  over  ...  \\\n",
      "0  Eden Gardens, Kolkata          KKR          RCB        1   0.1  ...   \n",
      "1  Eden Gardens, Kolkata          KKR          RCB        1   0.2  ...   \n",
      "2  Eden Gardens, Kolkata          KKR          RCB        1   0.3  ...   \n",
      "3  Eden Gardens, Kolkata          KKR          RCB        1   0.4  ...   \n",
      "4  Eden Gardens, Kolkata          KKR          RCB        1   0.5  ...   \n",
      "\n",
      "      bowler runs_of_bat  extras  wide  legbyes  byes  noballs  wicket_type  \\\n",
      "0  Hazlewood           0       0     0        0     0        0          NaN   \n",
      "1  Hazlewood           4       0     0        0     0        0          NaN   \n",
      "2  Hazlewood           0       0     0        0     0        0          NaN   \n",
      "3  Hazlewood           0       0     0        0     0        0          NaN   \n",
      "4  Hazlewood           0       0     0        0     0        0       caught   \n",
      "\n",
      "  player_dismissed        fielder  \n",
      "0              NaN            NaN  \n",
      "1              NaN            NaN  \n",
      "2              NaN            NaN  \n",
      "3              NaN            NaN  \n",
      "4          de Kock  Jitesh Sharma  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "drive_path = 'D:\\\\VS Codes\\\\Projects\\\\IPL-Score-Prediction-using-Deep-Learning-\\\\data\\\\raw'\n",
    "\n",
    "filenames = [\n",
    "    'ipl_2025_deliveries.csv',\n",
    "    'ipl_2024_deliveries.csv',\n",
    "    'ipl_2023_deliveries.csv',\n",
    "    'ipl_2022_deliveries.csv'\n",
    "]\n",
    "\n",
    "dataframes_to_combine = []\n",
    "\n",
    "# Ensure the directory exists before trying to read files\n",
    "if not os.path.exists(drive_path):\n",
    "    print(f\"❌ ERROR: Directory '{drive_path}' not found. Please check the path and ensure files are in Google Drive.\")\n",
    "else:\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(drive_path, filename)\n",
    "        try:\n",
    "            df_temp = pd.read_csv(file_path)\n",
    "            dataframes_to_combine.append(df_temp)\n",
    "            print(f\"✅ Dataset '{filename}' loaded successfully! Shape: {df_temp.shape}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ ERROR: File '{file_path}' not found. Skipping this file.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ ERROR: An unexpected error occurred while loading '{file_path}': {e}\")\n",
    "\n",
    "if dataframes_to_combine:\n",
    "    combined_df = pd.concat(dataframes_to_combine, ignore_index=True)\n",
    "    print(\"\\n✅ All available datasets combined successfully!\")\n",
    "    print(f\"Combined DataFrame shape: {combined_df.shape}\")\n",
    "    print(\"First 5 rows of the combined DataFrame:\")\n",
    "    print(combined_df.head())\n",
    "else:\n",
    "    print(\"\\nNo datasets were loaded to combine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1764395574634,
     "user": {
      "displayName": "Arsh AJ",
      "userId": "04800759354100934554"
     },
     "user_tz": -240
    },
    "id": "iLXZb1--IRQ8"
   },
   "outputs": [],
   "source": [
    "df = combined_df.copy()\n",
    "df = df[df['innings'].isin([1, 2])].copy()\n",
    "\n",
    "# Calculate 'total_runs_delivery' (runs off bat + extras) if not already present\n",
    "# Most Kaggle datasets have 'runs_of_bat' and 'extras'\n",
    "if 'total_runs' not in df.columns:\n",
    "    df['total_runs'] = df['runs_of_bat'] + df['extras']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1764395586676,
     "user": {
      "displayName": "Arsh AJ",
      "userId": "04800759354100934554"
     },
     "user_tz": -240
    },
    "id": "926wKvSAIWg8"
   },
   "outputs": [],
   "source": [
    "match_scores = df.groupby(['match_id', 'innings'])['total_runs'].sum().reset_index()\n",
    "match_scores.rename(columns={'total_runs': 'final_total_score'}, inplace=True)\n",
    "\n",
    "# Merge this back into the main dataframe\n",
    "# Now every ball knows what the final score of that inning eventually became.\n",
    "df = df.merge(match_scores, on=['match_id', 'innings'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1764395645808,
     "user": {
      "displayName": "Arsh AJ",
      "userId": "04800759354100934554"
     },
     "user_tz": -240
    },
    "id": "M-l6Ow0NIYEy"
   },
   "outputs": [],
   "source": [
    "# We need \"Current State\" features:\n",
    "# 1. Current Score (Cumulative Sum of runs)\n",
    "# 2. Wickets Fallen (Cumulative Count of dismissals)\n",
    "# 3. Balls Bowled (To calculate Overs)\n",
    "\n",
    "# Sort strictly by Match -> Innings -> Ball Number to ensure cumulative sum works\n",
    "df.sort_values(['match_id', 'innings', 'over'], inplace=True)\n",
    "\n",
    "# Cumulative Runs\n",
    "df['current_score'] = df.groupby(['match_id', 'innings'])['total_runs'].cumsum()\n",
    "\n",
    "# Cumulative Wickets\n",
    "# First, create a binary column: 1 if wicket fell, 0 if not\n",
    "df['is_wicket'] = df['player_dismissed'].apply(lambda x: 1 if pd.notnull(x) else 0)\n",
    "df['wickets_fallen'] = df.groupby(['match_id', 'innings'])['is_wicket'].cumsum()\n",
    "\n",
    "# Calculate Overs Completed (e.g., Ball 0.6 is 1.0 overs)\n",
    "# 'over' usually comes as 0.1, 0.2 ... 0.6. We can directly use this for 'overs'.\n",
    "df['overs'] = df['over']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1764395711503,
     "user": {
      "displayName": "Arsh AJ",
      "userId": "04800759354100934554"
     },
     "user_tz": -240
    },
    "id": "Dez3DA4ZIcED"
   },
   "outputs": [],
   "source": [
    "# Neural networks need numbers, not names like \"CSK\"\n",
    "le_teams = LabelEncoder()\n",
    "le_venue = LabelEncoder()\n",
    "\n",
    "# Fit on all teams (batting and bowling)\n",
    "all_teams = pd.concat([df['batting_team'], df['bowling_team']]).unique()\n",
    "le_teams.fit(all_teams)\n",
    "\n",
    "df['batting_team_encoded'] = le_teams.transform(df['batting_team'])\n",
    "df['bowling_team_encoded'] = le_teams.transform(df['bowling_team'])\n",
    "\n",
    "# Handle Venues\n",
    "# Fill missing venues just in case\n",
    "df['venue'] = df['venue'].fillna('Unknown')\n",
    "df['venue_encoded'] = le_venue.fit_transform(df['venue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1764395713341,
     "user": {
      "displayName": "Arsh AJ",
      "userId": "04800759354100934554"
     },
     "user_tz": -240
    },
    "id": "pmLnoQgfIdfm"
   },
   "outputs": [],
   "source": [
    "# We keep only what the model needs\n",
    "features = ['batting_team_encoded', 'bowling_team_encoded', 'venue_encoded',\n",
    "            'current_score', 'wickets_fallen', 'overs']\n",
    "target = ['final_total_score']\n",
    "\n",
    "# Drop rows where final score is missing (rare errors)\n",
    "model_data = df[features + target].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1764395715754,
     "user": {
      "displayName": "Arsh AJ",
      "userId": "04800759354100934554"
     },
     "user_tz": -240
    },
    "id": "v_T_nxyaIhJf",
    "outputId": "a151fce9-af15-423d-809e-fd26471673f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Data Cleaning Complete!\n",
      "Training Features shape: (69587, 6)\n",
      "Target Labels shape: (69587, 1)\n",
      "\n",
      "Example of Data (First 5 rows):\n",
      "       batting_team_encoded  bowling_team_encoded  venue_encoded  \\\n",
      "51675                     0                     3             17   \n",
      "51676                     0                     3             17   \n",
      "51677                     0                     3             17   \n",
      "51678                     0                     3             17   \n",
      "51679                     0                     3             17   \n",
      "\n",
      "       current_score  wickets_fallen  overs  final_total_score  \n",
      "51675              1               0    0.1                131  \n",
      "51676              1               0    0.1                131  \n",
      "51677              2               0    0.2                131  \n",
      "51678              2               0    0.2                131  \n",
      "51679              2               1    0.3                131  \n",
      "Saved processed dataset to: D:\\\\VS Codes\\\\Projects\\\\IPL-Score-Prediction-using-Deep-Learning-\\\\data\\\\processed\\model_data_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert to Numpy Arrays first\n",
    "X_raw = model_data[features].values\n",
    "y_raw = model_data[target].values\n",
    "\n",
    "# Scale the data (CRITICAL for Neural Networks!)\n",
    "# We scale Inputs (X) to be roughly between 0 and 1\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X_raw)\n",
    "\n",
    "# We usually don't scale Y for regression unless values are huge,\n",
    "# but for MDN, keeping Y in raw runs (e.g. 150, 200) is interpretable.\n",
    "\n",
    "print(\"\\n✅ Data Cleaning Complete!\")\n",
    "print(f\"Training Features shape: {X_scaled.shape}\")\n",
    "print(f\"Target Labels shape: {y_raw.shape}\")\n",
    "print(\"\\nExample of Data (First 5 rows):\")\n",
    "print(model_data.head())\n",
    "\n",
    "# Save the processed dataset to `data/processed`\n",
    "import os\n",
    "processed_dir = r'D:\\\\VS Codes\\\\Projects\\\\IPL-Score-Prediction-using-Deep-Learning-\\\\data\\\\processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "out_path = os.path.join(processed_dir, 'model_data_processed.csv')\n",
    "model_data.to_csv(out_path, index=False)\n",
    "print(f\"Saved processed dataset to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "557cf219"
   },
   "source": [
    "## Final Task\n",
    "\n",
    "### Subtask:\n",
    "Confirm that Google Drive has been successfully mounted and is accessible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42ec0d74"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "### Data Analysis Key Findings\n",
    "*   Google Drive was successfully mounted, confirming accessibility for file operations.\n",
    "*   Four IPL delivery CSV files were successfully loaded and combined:\n",
    "    *   `ipl_2025_deliveries.csv` (17,246 rows, 21 columns)\n",
    "    *   `ipl_2024_deliveries.csv` (17,053 rows, 20 columns)\n",
    "    *   `ipl_2023_deliveries.csv` (17,386 rows, 20 columns)\n",
    "    *   `ipl_2022_deliveries.csv` (17,912 rows, 20 columns)\n",
    "*   The combined DataFrame, `combined_df`, has a total of 69,597 rows and 21 columns.\n",
    "\n",
    "### Insights or Next Steps\n",
    "*   The combined dataset is ready for comprehensive analysis of IPL delivery data across the years 2022-2025, enabling year-over-year comparisons and trend identification.\n",
    "*   Further data cleaning, such as handling missing values or inconsistencies in columns (e.g., the difference in column count between `ipl_2025_deliveries.csv` and the other files), should be performed before detailed analysis.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNzC0a8uUoh0w63TlMGIt+4",
   "mount_file_id": "195-C56Nw8wuiYy1cZbpiRTfPEAYkj-8t",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
